# -*- coding: utf-8 -*-
"""VR - Modelo de Recomendação - Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1chMNSkBt6drNlfGg2qphhkOnay1qeyrN

# **VR - MODELO DE RECOMENDAÇÃO**

### **OBJETIVO:** Construir um sistema de recomendação de estabelecimentos para usuários que aceitaram receber recomendações, maximizando o lucro (taxa do estabelecimento × valor da transação).

Bases de dados:
*   usuarios.csv: dados dos usuários, incluindo flags de aceite.
*   transacoes.csv: histórico de transações por usuário e estabelecimento.
*   estabelecimento.csv: dados dos estabelecimentos, incluindo a taxa (comissão da VR).



Este notebook implementa um sistema de recomendação baseado no algoritmo LightFM.
O objetivo é recomendar estabelecimentos para trabalhadores com base em:
1. Padrões de interação passados (matriz de interações)
2. Características dos estabelecimentos (item features)
3. Características dos trabalhadores (user features)

O sistema usa a abordagem híbrida do LightFM, que combina filtragem colaborativa com informações de conteúdo, ideal para lidar com o problema de cold-start.

### 0.0 Importação de bibliotecas e configurações iniciais
"""

# Instalar bibliotecas
!pip install lightfm

# Manipulação de Dados
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from tqdm import tqdm
import time
from itertools import product

# Visualização
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go

# Machine Learning e Recomendação
from lightfm import LightFM
from lightfm.evaluation import precision_at_k, auc_score
from scipy import sparse
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_squared_error

# Estatísticas
from scipy.stats import ttest_ind, skew, norm
import math

# Utilitários
import warnings
warnings.filterwarnings('ignore')

# Para acesso ao Google Drive (se necessário)
from google.colab import drive

def plot_img(img_array, plot_axis=False):
    '''
    img_array: é o retorno da função `keras.utils.img_to_array` que retorna o array da imagem considerando o formato RGB
    '''
    img_pil = Image.fromarray(img_array.astype('uint8'))
    if plot_axis:
        return plt.imshow(img_pil)
    return img_pil


def plot_nn(model, settings={}):
    visualizer(model, file_name='/tmp/output', file_format='png', view=True, settings=settings)
    img = Image.open('/tmp/output.png')
    return img


def plot_random_imgs(df, rows=2, columns=5, figsize=(8, 4), show_predictions=False):
    fig, axs = plt.subplots(rows, columns, figsize=figsize)

    idx_img = list(np.random.choice(list(df.index), rows*columns, replace=False))
    print(idx_img)
    for i, ax in enumerate(axs.flat):
        title = f'{df.target.iloc[idx_img[i]]}'

        if show_predictions and 'predicted_class' in df.columns:
            title = title + f' | Pred:{df.predicted_class.iloc[idx_img[i]]} ( {df.target_proba.iloc[idx_img[i]]:.3f})'

        ax.imshow(plt.imread(df.full_path.iloc[idx_img[i]]))
        ax.set_title(title)
        ax.axis('off')
    plt.tight_layout()

def plot_history(history):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

    ax1.plot(history.history['loss'], 'r-', label='train loss')
    ax1.plot(history.history['val_loss'], 'b--', label='test loss')
    ax1.set_title('Model Loss')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Loss')

    ax1.legend()

    ax2.plot(history.history['accuracy'], 'r-', label='train acc')
    ax2.plot(history.history['val_accuracy'], 'b--', label='test acc')
    ax2.set_title('Model Accuracy')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('Accuracy')
    ax2.legend()

    plt.tight_layout()

def save_obj(obj, full_path):
    try:
        joblib.dump(obj, full_path)
    except Exception as e:
        print(e)

from google.colab import drive

drive.mount('/content/drive')

# Leitura da base de dados
estabelecimentos = pd.read_csv('/content/drive/My Drive/VR - Recomendação/estabelecimento.csv')
transacoes = pd.read_csv('/content/drive/My Drive/VR - Recomendação/transacoes.csv')
usuarios = pd.read_csv('/content/drive/My Drive/VR - Recomendação/usuarios.csv')

estabelecimentos.head()

# Avalia a formatação das colunas
estabelecimentos.dtypes

# Imprime as 5 primeiras linhas
transacoes.head()

# Avalia a formatação das colunas
transacoes.dtypes

# Imprime as 5 primeiras linhas
usuarios.head()

# Avalia a formatação das colunas
usuarios.dtypes

# Une os dataframes
transacoes_joined = (
    pd.merge(transacoes, estabelecimentos, on='id_ec', how='left')
    .merge(usuarios, on='id_trabalhador', how='left')
)

# Imprime as 5 primeiras linhas
transacoes_joined.head()

# Cálculo do lucro por transação: valor da transação multiplicado pela taxa que a VR retém do estabelecimento.
# Essa será a métrica principal usada no modelo de recomendação, pois representa o ganho da empresa por transação.
transacoes_joined['lucro'] = transacoes_joined['val_transacao_consumo'] * transacoes_joined['tx_empresa']

# Estilo visual
sns.set(style="whitegrid", font_scale=1.2)
plt.rcParams["figure.figsize"] = (12, 6)

colors = ['#00b433ff', '#004723ff', '#00da00ff', '#3efde8ff', '#a8ffc2ff']

"""## 1.0 Análise exploratória de dados (EDA)"""

# Filtra apenas os usuários que aceitaram receber recomendações e aceitaram os termos.
# Isso garante que o sistema só recomende para quem está elegível, conforme briefing do desafio.
total_usuarios = usuarios['id_trabalhador'].nunique()

usuarios_aceite = usuarios[
    (usuarios['aceita_recomendacao'] == 1) &
    (usuarios['aceita_termo'] == 1)
]

proporcao = len(usuarios_aceite) / total_usuarios

print(f"Proporção de usuários com aceite: {proporcao:.2%}")

# Mostra o volume de transações registradas após o merge.
# Dá uma noção da densidade da base e do potencial de lucro a ser modelado.
print(f"Total de transações: {transacoes_joined.shape[0]:,}")
print(f"Número de colunas: {transacoes_joined.shape[1]}")
print("\nColunas disponíveis:")
print(transacoes_joined.columns.tolist())

# Verificando tipos de dados e valores ausentes
transacoes_joined.info()
print("\nValores ausentes por coluna:")
print(transacoes_joined.isnull().sum())

"""Base completa, sem valores ausentes. Colunas estão em tipo correto."""

transacoes_joined = transacoes_joined[(transacoes_joined['aceita_recomendacao'] == 1) & (transacoes_joined['aceita_termo'] == 1)]
transacoes_joined = transacoes_joined.drop(columns=['aceita_recomendacao', 'aceita_termo'])

transacoes_joined.describe()

# Remove transações com valor ou taxa negativa, pois não representam lucro real para a empresa.
# Também serve como forma de limpeza de dados inconsistentes ou erros de registro.

# Transações negativas
negativas = transacoes_joined[transacoes_joined['val_transacao_consumo'] < 0]
print(f"Transações com valor negativo: {len(negativas)} ({100 * len(negativas) / len(transacoes_joined):.2f}%)")

# Transações com taxa 0 ou negativa
tx_zero = transacoes_joined[transacoes_joined['tx_empresa'] <= 0]
print(f"Transações com taxa <= 0: {len(tx_zero)}")

# Remove transações com valor negativo e taxa zero ou negativa
transacoes_clean = transacoes_joined[
    (transacoes_joined['val_transacao_consumo'] > 0) &
    (transacoes_joined['tx_empresa'] > 0)
].copy()

print(f"Transações após limpeza: {len(transacoes_clean):,}")

"""### 1.1. Usuários e estabelecimentos"""

# Conta o total de trabalhadores únicos na base.
# Ajuda a entender o tamanho da base de usuários que podem receber recomendações.
num_trabalhadores = transacoes_clean['id_trabalhador'].nunique()
print(f"Número de trabalhadores únicos: {num_trabalhadores:,}")

# Total de estabelecimentos únicos
num_estabelecimentos = transacoes_clean['id_ec'].nunique()
print(f"Número de estabelecimentos únicos: {num_estabelecimentos:,}")

# Esparsidade da matriz
num_total_interactions = transacoes_clean.shape[0]
sparsity = 1 - (num_total_interactions / (num_trabalhadores * num_estabelecimentos))
print(f"Densidade da matriz de interação (1 - esparsidade): {1 - sparsity:.4f}")
print(f"Esparsidade da matriz de interação: {sparsity:.4f}")

# Estabelecimentos por trabalhador
ecs_por_user = transacoes_clean.groupby('id_trabalhador')['id_ec'].nunique()

sns.histplot(ecs_por_user, bins=30, color=colors[0])
plt.title("Distribuição: Estabelecimentos únicos por Trabalhador")
plt.xlabel("Quantidade de Estabelecimentos")
plt.ylabel("Número de Trabalhadores")
plt.show()

"""Usuários tendem a visitar poucos estabelecimentos comerciais distintos (maior parte até 10), o que justifica a recomendação personalizada.

### 1.2. Transações e valores
"""

# Valor total e médio
valor_total = transacoes_clean['val_transacao_consumo'].sum()
valor_medio = transacoes_clean['val_transacao_consumo'].mean()
print(f"Valor total transacionado: R$ {valor_total:,.2f}")
print(f"Valor médio por transação: R$ {valor_medio:,.2f}")

# Filtrando os extremos para evitar distorção
q_low, q_high = transacoes_clean['val_transacao_consumo'].quantile([0.01, 0.99])
valores_filtrados = transacoes_clean[
    (transacoes_clean['val_transacao_consumo'] >= q_low) &
    (transacoes_clean['val_transacao_consumo'] <= q_high)
]

# Gráfico de linha de densidade
sns.kdeplot(valores_filtrados['val_transacao_consumo'], fill=True, color=colors[1], linewidth=2)
plt.title("Densidade dos Valores de Transação (1º–99º percentil)")
plt.xlabel("Valor da Transação (R$)")
plt.ylabel("Densidade")
plt.grid(True)
plt.show()

"""Removi os 1% extremos da distribuição para uma visualização mais fiel ao comportamento majoritário dos consumidores, evitando distorções causadas por outliers."""

plt.rcParams.update({'font.size': 10})
skewness = str(skew(transacoes_clean['val_transacao_consumo']))
sns.distplot(transacoes_clean['val_transacao_consumo'] ,fit = norm, color = colors[0])
plt.title("Assimetria de Valor Transação Consumo"+" = "+skewness)
plt.show()

# Cria um DataFrame com a coluna necessária para adicionar a nova coluna log
valores_log = transacoes_clean[['val_transacao_consumo']].copy()
valores_log['log_valor'] = np.log1p(valores_log['val_transacao_consumo'])  # log(1 + x) evita log(0)

# Plotando a densidade com escala log
sns.kdeplot(valores_log['log_valor'], fill=True, color=colors[2], linewidth=2)
plt.title("Densidade dos Valores de Transação (escala log)")
plt.xlabel("log(1 + Valor da Transação)")
plt.ylabel("Densidade")
plt.grid(True)
plt.show()

"""Valores seguem distribuição com cauda longa. Aplicação de log revela padrão de massa principal."""

transacoes_por_usuario = transacoes_clean.groupby('id_trabalhador').size()
sns.histplot(transacoes_por_usuario, bins=40, color=colors[0])
plt.title("Distribuição de transações por usuário")
plt.xlabel("Nº de transações")
plt.ylabel("Usuários")
plt.show()

"""Distribuição assimétrica, com alta concentração de usuários com baixo volume de transações."""

# Valor total por trabalhador
valor_por_trabalhador = transacoes_clean.groupby('id_trabalhador')['val_transacao_consumo'].sum()
print(f"Valor médio total por trabalhador: R$ {valor_por_trabalhador.mean():,.2f}")

"""### 1.3. Lucro"""

# Lucro total
lucro_total = transacoes_clean['lucro'].sum()
print(f"Lucro total estimado da VR: R$ {lucro_total:,.2f}")

# Lucro médio por transação
print(f"Lucro médio por transação: R$ {transacoes_clean['lucro'].mean():,.2f}")

# Calcular o lucro total por usuário
lucro_por_usuario = transacoes_clean.groupby('id_trabalhador')['lucro'].sum()

# Calcular o lucro médio por usuário
lucro_medio_por_usuario = lucro_por_usuario.mean()

print(f"Lucro médio por usuário: R$ {lucro_medio_por_usuario:,.2f}")

# Top 10 ECs mais lucrativos
top_lucro_ecs = transacoes_clean.groupby('id_ec')['lucro'].sum().sort_values(ascending=False).head(10)

sns.barplot(x=top_lucro_ecs.values, y=top_lucro_ecs.index, palette=colors)
plt.title("Top 10 Estabelecimentos Mais Lucrativos")
plt.xlabel("Lucro Total (R$)")
plt.ylabel("ID do Estabelecimento")
plt.show()

"""### 1.4. Temporalidade"""

# Certificar que a coluna 'dat_transacao' está em formato datetime
transacoes_clean['dat_transacao'] = pd.to_datetime(transacoes_clean['dat_transacao'])

# Primeiro e último registro
print("Período de transações:", transacoes_clean['dat_transacao'].min().date(), "→", transacoes_clean['dat_transacao'].max().date())

# Transações por dia
transacoes_por_dia = transacoes_clean.groupby(transacoes_clean['dat_transacao'].dt.date).size()
print("\nTransações por dia (amostra):")
print(transacoes_por_dia.tail())

transacoes_por_dia.plot(kind='line', color=colors[2])
plt.title("Volume de Transações por Dia")
plt.xlabel("Data")
plt.ylabel("Número de Transações")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Soma do valor por dia
valor_por_dia = transacoes_clean.groupby(transacoes_clean['dat_transacao'].dt.date)['val_transacao_consumo'].sum()

# Gráfico de linha
plt.figure(figsize=(12, 6))
sns.lineplot(x=valor_por_dia.index, y=valor_por_dia.values, color=colors[0])
plt.title("Valor Total de Transações por Dia")
plt.xlabel("Data")
plt.ylabel("Valor Total Transacionado (R$)")
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

# Soma do valor total por dia
valor_por_dia = transacoes_clean.groupby(transacoes_clean['dat_transacao'].dt.date)['val_transacao_consumo'].sum()

# Calcula o limite superior do percentil 99 (para remover picos extremos)
limite_superior = valor_por_dia.quantile(0.99)

# Filtra os dias com valor dentro do range razoável
valor_por_dia_filtrado = valor_por_dia[valor_por_dia <= limite_superior]

# Replota o gráfico com dados limpos
plt.figure(figsize=(12, 6))
sns.lineplot(x=valor_por_dia_filtrado.index, y=valor_por_dia_filtrado.values, color=colors[3])
plt.title("Valor Total de Transações por Dia (Sem Outliers)")
plt.xlabel("Data")
plt.ylabel("Valor Total Transacionado (R$)")
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

"""Valor e volume de transações possui sazonalidade clara, com maior concentração nos primeiros dias do mês. Condiz com o dia de pagamento dos trabalhadores, o que faz sentido pensando que é uma empresa de benefícios."""

# Calcular o limite superior para transações individuais (percentil 99999)
limite_superior_individual = transacoes_clean['val_transacao_consumo'].quantile(0.99999)

# Remover apenas as transações acima desse limite
transacoes_sem_outliers_individuais = transacoes_clean[transacoes_clean['val_transacao_consumo'] <= limite_superior_individual]

print(f"Tamanho original da base: {len(transacoes_clean)}")
print(f"Tamanho após remover transações outliers: {len(transacoes_sem_outliers_individuais)}")
print(f"Número de transações removidas: {len(transacoes_clean) - len(transacoes_sem_outliers_individuais)}")

transacoes_clean = transacoes_sem_outliers_individuais

# Soma do valor por dia
valor_por_dia = transacoes_clean.groupby(transacoes_clean['dat_transacao'].dt.date)['val_transacao_consumo'].sum()

# Gráfico de linha
plt.figure(figsize=(12, 6))
sns.lineplot(x=valor_por_dia.index, y=valor_por_dia.values, color=colors[0])
plt.title("Valor Total de Transações por Dia")
plt.xlabel("Data")
plt.ylabel("Valor Total Transacionado (R$)")
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

# Top 10 ECs mais lucrativos - refazer a visualização sem esse outlier
top_lucro_ecs = transacoes_clean.groupby('id_ec')['lucro'].sum().sort_values(ascending=False).head(10)

sns.barplot(x=top_lucro_ecs.values, y=top_lucro_ecs.index, palette=colors)
plt.title("Top 10 Estabelecimentos Mais Lucrativos")
plt.xlabel("Lucro Total (R$)")
plt.ylabel("ID do Estabelecimento")
plt.show()

# Distribuição de lucro por EC (concentração)
lucro_por_ec = transacoes_clean.groupby('id_ec')['lucro'].sum().sort_values(ascending=False)

# Quantos ECs concentram 80% do lucro?
cumsum = lucro_por_ec.cumsum()
total_lucro = lucro_por_ec.sum()
porcentagem = cumsum / total_lucro
print(f"ECs que concentram 80% do lucro: {(porcentagem < 0.8).sum()} estabelecimentos")

"""### 1.5. Demografia"""

# Idade
sns.histplot(transacoes_clean['dsc_idade'], bins=30, kde=True, color=colors[3])
plt.title("Distribuição de Idade dos Trabalhadores")
plt.xlabel("Idade")
plt.ylabel("Frequência")
plt.show()

"""Concentração em jovens adultos e adultos (30 e 60 anos). Também condizente com o produto da empresa, representa bem a classe trabalhadora."""

# Conta as ocorrências
sexo_counts = transacoes_clean['dsc_sexo'].value_counts()
total = sexo_counts.sum()

# Plota o gráfico
plt.figure(figsize=(10, 8))
ax = sns.countplot(x='dsc_sexo', data=transacoes_clean, palette=colors)

# Adiciona os rótulos nas barras
for p in ax.patches:
    count = int(p.get_height())
    perc = 100 * count / total
    ax.annotate(f'{count:,}\n({perc:.1f}%)',
                (p.get_x() + p.get_width() / 2, p.get_height()),
                ha='center', va='bottom', fontsize=11)

# Títulos e eixos
plt.title("Distribuição por Sexo")
plt.xlabel("Sexo")
plt.ylabel("Número de Transações")
plt.tight_layout()
plt.show()

"""A maioria não informou o gênero, mas com a pouca informação disponível, pode-se dizer que é equilibrada em gênero.

## 2.0 Engenharia de features

### 2.1. Features temporais
"""

# Certifica que a data está no formato datetime
transacoes_clean['dat_transacao'] = pd.to_datetime(transacoes_clean['dat_transacao'])

# Extrai partes da data
transacoes_clean['ano'] = transacoes_clean['dat_transacao'].dt.year
transacoes_clean['mes'] = transacoes_clean['dat_transacao'].dt.month
transacoes_clean['dia'] = transacoes_clean['dat_transacao'].dt.day
transacoes_clean['dia_semana'] = transacoes_clean['dat_transacao'].dt.dayofweek  # 0 = segunda
transacoes_clean['semana_mes'] = transacoes_clean['dia'].apply(lambda x: (x - 1) // 7 + 1)

transacoes_clean.head()

"""### 2.2. Separação em treino e teste"""

# Ordena pelas datas de transação
transacoes_clean = transacoes_clean.sort_values('dat_transacao')

# Define ponto de corte temporal (80% do tempo para treino)
split_date = transacoes_clean['dat_transacao'].quantile(0.8)

# Divide entre treino e teste
transacoes_train = transacoes_clean[transacoes_clean['dat_transacao'] <= split_date]
transacoes_test = transacoes_clean[transacoes_clean['dat_transacao'] > split_date]

# Exibe informações do split
print(f"Data de corte (split_date): {split_date}")
print(f"Transações de treino: {transacoes_train.shape[0]:,}")
print(f"Transações de teste: {transacoes_test.shape[0]:,}")

"""### 2.2 Agregações por usuário"""

# Ordena e calcula recência por usuário
transacoes_train = transacoes_train.sort_values(['id_trabalhador', 'dat_transacao'])
transacoes_train['recencia_dias'] = transacoes_train.groupby('id_trabalhador')['dat_transacao'].diff().dt.days

# Agregações por usuário
user_agg = transacoes_train.groupby('id_trabalhador').agg(
    num_transacoes_us=('val_transacao_consumo', 'count'),
    total_gasto_us=('val_transacao_consumo', 'sum'),
    gasto_medio_us=('val_transacao_consumo', 'mean'),
    total_lucro_us=('lucro', 'sum'),
    lucro_medio_us=('lucro', 'mean'),
    num_ecs_distintos_us=('id_ec', 'nunique'),
    recencia_media_us=('recencia_dias', 'mean')
).reset_index()

"""### 2.3. Agregações por estabelecimento"""

# Agregações por estabelecimento
ec_agg = transacoes_train.groupby('id_ec').agg(
    num_transacoes_ec=('val_transacao_consumo', 'count'),
    total_receita_ec=('val_transacao_consumo', 'sum'),
    receita_media_ec=('val_transacao_consumo', 'mean'),
    total_lucro_ec=('lucro', 'sum'),
    lucro_medio_ec=('lucro', 'mean'),
    num_usuarios_distintos_ec=('id_trabalhador', 'nunique')
).reset_index()

"""### 2.4. Agregações por usuário e estabelecimento"""

# Agregações por usuário x EC
user_ec_agg = transacoes_train.groupby(['id_trabalhador', 'id_ec']).agg(
    num_visitas_us_ec=('val_transacao_consumo', 'count'),
    total_gasto_us_ec=('val_transacao_consumo', 'sum'),
    ticket_medio_us_ec=('val_transacao_consumo', 'mean'),
    total_lucro_us_ec=('lucro', 'sum'),
    lucro_medio_us_ec=('lucro', 'mean')
).reset_index()

"""### 2.5. Agregações finais e normalização"""

from sklearn.preprocessing import MinMaxScaler

# Demográficos únicos por usuário
demograficos = transacoes_train[['id_trabalhador', 'dsc_idade', 'dsc_sexo']].drop_duplicates()

# Junta com agregações
user_features = pd.merge(user_agg, demograficos, on='id_trabalhador', how='left')

# One-hot encoding do sexo
user_features = pd.get_dummies(user_features, columns=['dsc_sexo'], prefix='sexo')

# Preenchendo possíveis NaNs gerados no merge ou agregações (só por segurança)
user_features.fillna(0, inplace=True)

# Normalizar variáveis numéricas
scaler = MinMaxScaler()

user_cols_to_scale = [
    'dsc_idade', 'num_transacoes_us', 'total_gasto_us', 'gasto_medio_us',
    'total_lucro_us', 'lucro_medio_us', 'num_ecs_distintos_us', 'recencia_media_us'
]

user_features[user_cols_to_scale] = scaler.fit_transform(user_features[user_cols_to_scale])

# Estabelecimentos — já temos o ec_agg
item_features = ec_agg.copy()

# Normalizar as variáveis numéricas dos ECs
item_cols_to_scale = [
    'num_transacoes_ec', 'total_receita_ec', 'receita_media_ec',
    'total_lucro_ec', 'lucro_medio_ec', 'num_usuarios_distintos_ec'
]

item_features[item_cols_to_scale] = scaler.fit_transform(item_features[item_cols_to_scale])

# Verificando formato final
print("User features shape:", user_features.shape)
print("Item features shape:", item_features.shape)
print("User features columns:", user_features.columns.tolist())
print("Item features columns:", item_features.columns.tolist())

"""## 3.0 Montagem para LightFM

### 3.1. Mapear os IDs
"""

# Mapeamento de usuários
user_features['user_idx'] = user_features['id_trabalhador'].astype('category').cat.codes
user_id_map = dict(zip(user_features['id_trabalhador'], user_features['user_idx']))

# Mapeamento de estabelecimentos
item_features['item_idx'] = item_features['id_ec'].astype('category').cat.codes
item_id_map = dict(zip(item_features['id_ec'], item_features['item_idx']))

"""### 3.2. Matriz de interação"""

# Junta os índices mapeados
user_ec_agg['user_idx'] = user_ec_agg['id_trabalhador'].map(user_id_map)
user_ec_agg['item_idx'] = user_ec_agg['id_ec'].map(item_id_map)

# Remove interações que não conseguiram ser mapeadas (por segurança)
interacao_filtrada = user_ec_agg.dropna(subset=['user_idx', 'item_idx'])

from scipy.sparse import coo_matrix

interactions = coo_matrix((
    interacao_filtrada['total_lucro_us_ec'],
    (interacao_filtrada['user_idx'].astype(int), interacao_filtrada['item_idx'].astype(int))
))

"""### 3.3. Criar o dataset"""

from lightfm.data import Dataset

dataset = Dataset()

# Consegue a lista de nome das features de estabelecimento (excluindo as colunas de ID)
item_feature_names = [col for col in item_features.columns if col not in ['id_ec', 'item_idx']]

# Consegue a lista de nome das features de usuário (excluindo as colunas de ID)
user_feature_names = [col for col in user_features.columns if col not in ['id_trabalhador', 'user_idx']]

# Fit no dataset com os IDs de usuários e estabelecimentos E features
dataset.fit(
    users=user_features['id_trabalhador'].tolist(),
    items=item_features['id_ec'].tolist(),
    item_features=item_feature_names,  # Include item feature names here
    user_features=user_feature_names   # Include user feature names here
)

# Criando as interações com pesoss
interactions, weights = dataset.build_interactions([
    (row['id_trabalhador'], row['id_ec'], row['num_visitas_us_ec'])
    for index, row in interacao_filtrada.iterrows()
])

# Criando a matriz de features para estabelecimentos
item_features_data = []
for index, row in item_features.iterrows():
    item_id = row['id_ec']
    # For each item, provide ALL the item feature names that were used in dataset.fit
    features_for_item = [col for col in item_features.columns if col not in ['id_ec', 'item_idx']]
    item_features_data.append((item_id, features_for_item))

item_features_matrix = dataset.build_item_features(item_features_data, normalize=False)

# Criando a matriz de features para trabalhadores
user_features_data = []
for index, row in user_features.iterrows():
    user_id = row['id_trabalhador']
    features_for_user = [col for col in user_features.columns if col not in ['id_trabalhador', 'user_idx']]
    user_features_data.append((user_id, features_for_user))


user_features_matrix = dataset.build_user_features(user_features_data, normalize=False)


# Imprime os tamanhos para validação
print("Interactions matrix shape:", interactions.shape)
print("Item features matrix shape:", item_features_matrix.shape)
print("User features matrix shape:", user_features_matrix.shape)

"""## 4.0 Modelagem com LightFM

**Justificativa da Escolha do LightFM**

Para este desafio de recomendação, o algoritmo LightFM foi escolhido por ser uma abordagem híbrida, o que o torna particularmente adequado para o nosso contexto na VR, pelas seguintes razões:

1.  **Natureza Híbrida:** O LightFM combina o melhor de dois mundos:
    * **Filtragem Colaborativa:** Aprende padrões de interação (visitas) entre usuários e estabelecimentos. Se um usuário similar ao "João" gostou do "Restaurante X", "Maria" (similar ao "João") também pode gostar. Isso é capturado pela matriz `interactions` baseada no `num_visitas_us_ec`.
    * **Informações de Conteúdo (Content-Based):** Utiliza características (features) tanto dos usuários (idade, sexo, histórico agregado de gastos e lucro) quanto dos estabelecimentos (número de transações, receita, lucro do EC). Isso é crucial, especialmente para:
        * **Problema de Cold-Start (Novos Usuários/Itens):** Mesmo que um novo usuário ou estabelecimento tenha poucas interações passadas, o modelo pode fazer recomendações inteligentes baseadas em suas características, mitigando o problema.
        * **Recomendações Diversas e Contextuais:** Permite que o modelo encontre conexões que apenas a filtragem colaborativa não veria (ex: recomendar um novo restaurante japonês a um usuário que gosta de comida asiática e tem perfil de alto gasto, mesmo que o usuário não tenha visitado muitos restaurantes japoneses antes).

    **1.1. Lida Eficazmente com a Esparsidade dos Dados:**
    A base de dados de interações entre trabalhadores e estabelecimentos da VR é inerentemente **esparsa** (como demonstrado na EDA, com uma esparsidade de **0.9996**). Isso significa que cada usuário interagiu com apenas uma pequena fração dos estabelecimentos disponíveis. Modelos puramente colaborativos teriam dificuldade em aprender padrões robustos com tão poucos dados de interação direta. A natureza híbrida do LightFM, ao incorporar features de usuários e itens, permite que o modelo generalize o aprendizado a partir dessas características ricas, preenchendo as lacunas da matriz esparsa e fazendo recomendações mais informadas mesmo sem interações diretas abundantes.

2.  **Otimização para Feedback Implícito:** O LightFM é otimizado para lidar com feedback implícito (como transações/visitas, onde não há uma "nota" explícita). As funções de perda como `WARP` (Weighted Approximate-Rank Pairwise) são ideais para esta tarefa, pois focam em rankear itens que o usuário provavelmente gostará acima dos que não gostará. Nossa escolha de `loss='warp'` reflete essa otimização.

3.  **Flexibilidade e Escalabilidade:** A biblioteca é eficiente e escalável para grandes conjuntos de dados, permitindo a inclusão de diversas features e um treinamento relativamente rápido.

4.  **Foco no Lucro:** Embora o `WARP` otimize o ranking por preferência, a inclusão de features relacionadas ao lucro (`total_lucro_us`, `lucro_medio_us` para usuários e `total_lucro_ec`, `lucro_medio_ec` para estabelecimentos), e a ponderação dessas features durante o treinamento (`user_feature_weights`, `item_feature_weights`), direcionam o modelo para priorizar recomendações que tendem a gerar maior lucro para a VR.

### 4.1. Inicialização e treinamento do modelo

*   loss='warp': Otimizado para feedback implícito e ranking (coloca itens preferidos no topo).
*   no_components=50: Dimensionalidade do espaço latente. Um bom equilíbrio entre complexidade e generalização.
*   random_state=42: Garante a reprodutibilidade dos resultados do treinamento.
*   item_alpha, user_alpha: Termos de regularização L2 para os embeddings de item e usuário. Ajudam a prevenir overfitting, forçando o modelo a aprender representações mais generalizáveis.
*   learning_rate: Taxa de aprendizado do algoritmo.
epochs=50: Número de épocas de treinamento. Aumentado para permitir maior convergência.
*   num_threads=4: Utiliza 4 threads para acelerar o treinamento.
"""

model = LightFM(loss='warp', no_components=50, random_state=42,
                item_alpha=1e-6, user_alpha=1e-6, learning_rate=0.03)
model.fit(
    interactions, # Matriz de interações (baseada em num_visitas_us_ec)
    user_features=user_features_matrix, # Matriz de features dos usuários
    item_features=item_features_matrix, # Matriz de features dos estabelecimentos
    user_feature_weights=user_feature_weights_array, # Pesos para features de usuário
    item_feature_weights=item_feature_weights_array, # Pesos para features de item
    epochs=50,
    num_threads=4
)

"""### 4.2. Aplicação na base de teste"""

# 1. Preparar dados do TESTE para avaliação do LightFM
# Mapear os IDs do teste para os índices do modelo (já existentes no treino)
transacoes_test_mapped = transacoes_test.copy()
transacoes_test_mapped['user_idx'] = transacoes_test_mapped['id_trabalhador'].map(user_id_map)
transacoes_test_mapped['item_idx'] = transacoes_test_mapped['id_ec'].map(item_id_map)

# Remover transações do teste onde o usuário ou item não foram vistos no treino (cold-start)
# LightFM só pode prever para usuários/itens que ele já viu no treino.
transacoes_test_mapped_filtered = transacoes_test_mapped.dropna(subset=['user_idx', 'item_idx'])

# Para construir a matriz de interações de TESTE para as métricas, precisamos de num_visitas_us_ec no teste.
# Ou, podemos apenas usar uma interação binária (1 para presente).
# Para manter a consistência com o peso usado no treino (num_visitas_us_ec), vamos agregá-lo para o teste.
user_ec_agg_test = transacoes_test_mapped_filtered.groupby(['id_trabalhador', 'id_ec']).agg(
    num_visitas_us_ec=('val_transacao_consumo', 'count')
).reset_index()

test_interactions, _ = dataset.build_interactions([
    (row['id_trabalhador'], row['id_ec'], row['num_visitas_us_ec'])
    for index, row in user_ec_agg_test.iterrows()
])


# 2. Calcular Precision e AUC
from lightfm.evaluation import precision_at_k, auc_score

# Definir k para as métricas
k_value = 5

# Precision@K
test_precision = precision_at_k(
    model,
    test_interactions, # Matriz de interações do teste
    user_features=user_features_matrix,
    item_features=item_features_matrix,
    k=k_value,
    num_threads=4
).mean() # Média da precisão para todos os usuários

# AUC score
test_auc = auc_score(
    model,
    test_interactions, # Matriz de interações do teste
    user_features=user_features_matrix,
    item_features=item_features_matrix,
    num_threads=4
).mean() # Média do AUC para todos os usuários

print(f"\nMétricas Adicionais (Avaliação de Ranking):")
print(f"Precision@{k_value}: {test_precision:.4f}")
print(f"AUC Score: {test_auc:.4f}")

"""### 4.3. Mapeamento de cold-start"""

# Percentual de usuários do teste que estão fora do treino
total_teste = transacoes_test['id_trabalhador'].nunique()
no_treino = len([uid for uid in transacoes_test['id_trabalhador'].unique() if uid not in user_id_map])
print(f"Usuários do teste fora do modelo: {no_treino}/{total_teste} ({100 * no_treino / total_teste:.2f}%)")

print(f"Número de usuários no teste: {transacoes_test['id_trabalhador'].nunique()}")
print(f"Número total de transações no teste: {len(transacoes_test)}")
print(f"Número médio de transações por usuário no teste: {transacoes_test.groupby('id_trabalhador').size().mean()}")

ecs_test = transacoes_test['id_ec'].unique()
ecs_train_mapped = item_features['id_ec'].unique() # ECs que o modelo conhece
cold_start_ecs_in_test = [ec for ec in ecs_test if ec not in ecs_train_mapped]
print(f"\nTotal de ECs únicos no teste: {len(ecs_test)}")
print(f"ECs no teste que o modelo NÃO viu no treino (cold-start): {len(cold_start_ecs_in_test)}")
print(f"Proporção de ECs cold-start no teste: {len(cold_start_ecs_in_test) / len(ecs_test):.2%}")

"""### 4.4. Recomendação para usuários da base de teste

Este bloco itera sobre os usuários do conjunto de teste para gerar suas TOP N recomendações (neste notebook, usei 5). As recomendações são baseadas nas previsões do modelo LightFM para estabelecimentos que o usuário ainda não visitou no período de treino.
"""

# Vamos usar os usuários do conjunto de teste
usuarios_teste = transacoes_test['id_trabalhador'].unique()

# IDs mapeados (filtrar apenas quem está no modelo)
usuarios_teste_idx = [
    user_id_map[uid] for uid in usuarios_teste if uid in user_id_map
]

# Para cada usuário, recomendar os TOP N estabelecimentos
N = 5
recomendacoes = {}

for uid in usuarios_teste:
    if uid not in user_id_map:
        continue  # usuário não está no treino

    user_idx = user_id_map[uid]

    # Estabelecimentos que ele ainda não visitou no treino
    ecs_treino = transacoes_train[transacoes_train['id_trabalhador'] == uid]['id_ec'].unique()
    ecs_possiveis = item_features['id_ec'][~item_features['id_ec'].isin(ecs_treino)].tolist()

    if not ecs_possiveis:
        continue  # usuário já visitou todos

    item_idxs = [item_id_map[i] for i in ecs_possiveis]

    scores = model.predict(user_idx, item_idxs, user_features=user_features_matrix, item_features=item_features_matrix)

    top_n = np.argsort(scores)[::-1][:N]
    recomendados = [ecs_possiveis[i] for i in top_n]

    recomendacoes[uid] = recomendados

"""### 4.5. Avaliação de lucro do modelo

Avalia o lucro REAL (coincidência com transações do usuário) do modelo vs uma baseline DE RECOMENDAÇÃO ALEATÓRIA POR USUÁRIO.

**Justificativa da Métrica de Avaliação:**

Esta função calcula o lucro REAL gerado pelo modelo, ou seja, o lucro proveniente das transações em que o usuário *realmente* visitou um dos estabelecimentos recomendados.

Essa é a métrica mais importante para o objetivo de negócio da VR, pois mede diretamente o impacto das recomendações no resultado financeiro da empresa.

A comparação com uma baseline de recomendação aleatória por usuário é fundamental para validar a capacidade do modelo de superar uma estratégia não inteligente. Um ganho significativo sobre a aleatoriedade demonstra que o modelo aprendeu padrões relevantes e consegue influenciar o comportamento do usuário de forma lucrativa.

Outras métricas, como Precision@K e AUC, medem a qualidade do ranking e a capacidade do modelo de prever interações, mas não o impacto direto no lucro. Por isso, o foco principal desta avaliação é o lucro real.
"""

def avaliar_modelo_lucro_real_aleatorio(model, transacoes_train, transacoes_test, user_features_matrix, item_features_matrix, user_id_map, item_id_map, user_features, item_features, N=5):
    """
    Avalia o lucro REAL (coincidência com transações do usuário) do modelo
    vs uma baseline DE RECOMENDAÇÃO ALEATÓRIA POR USUÁRIO.
    """
    print("Iniciando avaliação do Lucro REAL (com Baseline Aleatória por Usuário)...\n")

    # Pegar todos os ECs que o modelo conhece (que estão no item_id_map)
    all_known_ecs = list(item_id_map.keys())

    lucro_modelo_real = 0
    lucro_baseline_real = 0
    total_usuarios_avaliados = 0

    usuarios_teste = transacoes_test['id_trabalhador'].unique()

    # --- Debug para Usuários Específicos ---
    np.random.seed(42) # Para reprodutibilidade do debug (dos usuários selecionados)
    debug_users_sample = np.random.choice(usuarios_teste, min(5, len(usuarios_teste)), replace=False)

    for uid in usuarios_teste:
        if uid not in user_id_map:
            continue

        user_idx = user_id_map[uid]

        ecs_treino = transacoes_train[transacoes_train['id_trabalhador'] == uid]['id_ec'].unique()
        ecs_possiveis = item_features['id_ec'][~item_features['id_ec'].isin(ecs_treino)].tolist()

        # Filtra apenas os ECs possíveis que estão no item_id_map (ou seja, que o modelo conhece)
        # E que o usuário ainda não visitou no treino
        ecs_possiveis_para_recomendar = [i for i in ecs_possiveis if i in item_id_map]

        if not ecs_possiveis_para_recomendar:
            continue

        # --- Geração da Baseline Aleatória para ESTE USUÁRIO ---
        # Garantir que a baseline aleatória também não inclua ECs que o usuário já visitou no treino
        # e que sejam do universo de ECs conhecidos pelo modelo.
        # Se ecs_possiveis_para_recomendar tiver menos que N itens, pegamos o máximo possível.
        num_ecs_to_sample = min(N, len(ecs_possiveis_para_recomendar))
        np.random.seed(hash(uid) % (2**32 - 1)) # Nova seed para cada usuário para diferentes aleatoriedades, mas reprodutível
        top_ecs_baseline_aleatoria = np.random.choice(ecs_possiveis_para_recomendar, num_ecs_to_sample, replace=False).tolist()


        # Previsão do modelo
        item_idxs_for_prediction = [item_id_map[i] for i in ecs_possiveis_para_recomendar]

        scores = model.predict(user_idx, np.array(item_idxs_for_prediction),
                               user_features=user_features_matrix,
                               item_features=item_features_matrix)

        top_idxs = np.argsort(scores)[::-1][:N]
        top_ecs_modelo = [ecs_possiveis_para_recomendar[i] for i in top_idxs]

        # --- CÁLCULO DE LUCRO REAL (Coincidência com transações do usuário) ---
        # Filtra as transações reais feitas no teste para o usuário atual
        transacoes_usuario = transacoes_test[transacoes_test['id_trabalhador'] == uid]

        # Lucro se ele tivesse seguido a recomendação do modelo (coincidência real)
        lucro_mod_user_real = transacoes_usuario[transacoes_usuario['id_ec'].isin(top_ecs_modelo)]['lucro'].sum()

        # Lucro da baseline aleatória (coincidência real)
        lucro_base_user_real = transacoes_usuario[transacoes_usuario['id_ec'].isin(top_ecs_baseline_aleatoria)]['lucro'].sum()


        # --- Debug Detalhado para Usuários Amostrados ---
        if uid in debug_users_sample:
            print(f"\n--- DEBUG DETALHADO PARA USUÁRIO: {uid} (LUCRO REAL com BASELINE ALEATÓRIA POR USUÁRIO) ---")
            print(f"Transações REAIS (id_ec) do usuário no TESTE: {transacoes_usuario['id_ec'].unique().tolist()}")
            print(f"Recomendações do MODELO ({N}): {top_ecs_modelo}")
            print(f"Recomendações da BASELINE ALEATÓRIA POR USUÁRIO ({N}): {top_ecs_baseline_aleatoria}")
            print(f"Lucro REAL do MODELO (para este usuário): R$ {lucro_mod_user_real:,.2f}")
            print(f"Lucro REAL da BASELINE ALEATÓRIA (para este usuário): R$ {lucro_base_user_real:,.2f}")
            print("------------------------------------------")


        lucro_modelo_real += lucro_mod_user_real
        lucro_baseline_real += lucro_base_user_real
        total_usuarios_avaliados += 1

    print(f"\nUsuários avaliados (Lucro REAL com Baseline Aleatória): {total_usuarios_avaliados:,}")
    print(f"Lucro total (modelo - REAL): R$ {lucro_modelo_real:,.2f}")
    print(f"Lucro total (baseline - REAL - ALEATÓRIA POR USUÁRIO): R$ {lucro_baseline_real:,.2f}")

    # Atenção: Se lucro_baseline_real for 0, o ganho percentual dará NaN ou Inf.
    if lucro_baseline_real == 0:
        ganho_percentual_real = float('inf') if lucro_modelo_real > 0 else 0.0
    else:
        ganho_percentual_real = 100 * (lucro_modelo_real - lucro_baseline_real) / lucro_baseline_real

    print(f"Ganho percentual do modelo vs baseline (REAL - ALEATÓRIA POR USUÁRIO): {ganho_percentual_real:.2f}%")

    return {
        'lucro_modelo_real': lucro_modelo_real,
        'lucro_baseline_real': lucro_baseline_real,
        'ganho_percentual_real': ganho_percentual_real,
        'usuarios_avaliados': total_usuarios_avaliados
    }

avaliar_modelo_lucro_real_aleatorio(
    model,
    transacoes_train,
    transacoes_test,
    user_features_matrix,
    item_features_matrix,
    user_id_map,
    item_id_map,
    user_features,
    item_features,
    N=5  # top 5 recomendações
)

"""### 4.6. Curva K"""

def calcular_curva_lucro_real_por_k(model, transacoes_train, transacoes_test, user_features_matrix, item_features_matrix, user_id_map, item_id_map, user_features, item_features, Ks=[1, 3, 5, 10, 15, 20]):
    """
    Calcula o lucro REAL do modelo para diferentes valores de K (número de recomendações).
    Não calcula Precision/AUC aqui, pois estas são calculadas separadamente e de forma otimizada.
    """
    resultados = {'lucro_real': [], 'k_values': []}

    for K_current in Ks:
        print(f"\nAvaliando Lucro REAL para K = {K_current}...")

        lucro_modelo_real = 0
        total_usuarios_para_lucro_real = 0

        usuarios_teste_para_lucro_real = transacoes_test['id_trabalhador'].unique()

        for uid in usuarios_teste_para_lucro_real:
            if uid not in user_id_map:
                continue

            user_idx = user_id_map[uid]

            ecs_treino = transacoes_train[transacoes_train['id_trabalhador'] == uid]['id_ec'].unique()
            ecs_possiveis = item_features['id_ec'][~item_features['id_ec'].isin(ecs_treino)].tolist()
            ecs_possiveis_para_recomendar = [i for i in ecs_possiveis if i in item_id_map]

            if not ecs_possiveis_para_recomendar:
                continue

            item_idxs_for_prediction = [item_id_map[i] for i in ecs_possiveis_para_recomendar]

            scores = model.predict(user_idx, np.array(item_idxs_for_prediction),
                                   user_features=user_features_matrix,
                                   item_features=item_features_matrix)

            top_idxs = np.argsort(scores)[::-1][:K_current]
            top_ecs_modelo = [ecs_possiveis_para_recomendar[i] for i in top_idxs]

            # --- CÁLCULO DE LUCRO REAL (Coincidência com transações do usuário) ---
            transacoes_usuario = transacoes_test[transacoes_test['id_trabalhador'] == uid]
            lucro_mod_user_real = transacoes_usuario[transacoes_usuario['id_ec'].isin(top_ecs_modelo)]['lucro'].sum()
            lucro_modelo_real += lucro_mod_user_real
            total_usuarios_para_lucro_real += 1


        resultados['lucro_real'].append(lucro_modelo_real)
        resultados['k_values'].append(K_current)

        print(f"  Lucro total (modelo - REAL) @{K_current}: R$ {lucro_modelo_real:,.2f}")

    return resultados

print("\n--- Avaliando Curvas de Lucro REAL por K ---")
curva_resultados_lucro = calcular_curva_lucro_real_por_k(
    model,
    transacoes_train,
    transacoes_test,
    user_features_matrix,
    item_features_matrix,
    user_id_map,
    item_id_map,
    user_features,
    item_features,
    Ks=[1, 3, 5, 10, 15, 20]
)

# Plotar o lucro real por K
plt.figure(figsize=(10, 6))
plt.plot(curva_resultados_lucro['k_values'], curva_resultados_lucro['lucro_real'], marker='o', label='Lucro Real do Modelo')
plt.xlabel('K (Número de Recomendações)')
plt.ylabel('Lucro Real Total (R$)')
plt.title('Curva de Lucro Real do Modelo por K')
plt.grid(True)
plt.legend()
plt.xticks(curva_resultados_lucro['k_values'])
plt.tight_layout()
plt.show()